{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The following are the libraries required to perform various functions such as\n",
        "creating tf-idf and BOW values and also using logistic and linear regression on\n",
        "them\n"
      ],
      "metadata": {
        "id": "YmOaRQNDugGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "gFX6NqYh0dIO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function loads in the dataset such that the program can use it. The\n",
        "goal is to take in the dataset and return the training and testing data in a\n",
        "DataFrame. This process is supported by the Pandas library.\n"
      ],
      "metadata": {
        "id": "VYN9bA8zuihV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_train_test_imdb_data(data_dir):\n",
        "  data = {}\n",
        "  for dataset in [\"train\", \"test\"]:\n",
        "    data[dataset] = []\n",
        "    for sentiment in [\"neg\", \"pos\"]:\n",
        "      score = 1 if sentiment == \"pos\" else 0\n",
        "      path = os.path.join(data_dir, dataset, sentiment)\n",
        "      file_names = os.listdir(path)\n",
        "      for f_name in file_names:\n",
        "        with open(os.path.join(path, f_name), \"r\") as f:\n",
        "          review = f.read()\n",
        "          data[dataset].append([review, score])\n",
        "  data[\"train\"] = pd.DataFrame(data[\"train\"], columns=['text', 'sentiment'])\n",
        "  print(\"Training Data: \\n\")\n",
        "  print(data[\"train\"])\n",
        "  print(\"\\n\")\n",
        "  data[\"test\"] = pd.DataFrame(data[\"test\"], columns=['text', 'sentiment'])\n",
        "  print(\"Testing Data: \\n\")\n",
        "  print(data[\"test\"])\n",
        "  return data[\"train\"], data[\"test\"]\n",
        "\n",
        "train_data, test_data = load_train_test_imdb_data(data_dir=\"aclImdb/\")\n"
      ],
      "metadata": {
        "id": "9Zm8dfqb1QBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the function that cleans the text.\n",
        "The following operations are done on it:\n",
        "- Remove HTML tags\n",
        "- Remove punctuation\n",
        "- Putting all characters to its lowercase form\n",
        "- Removing words that are irrelevant to the sentiment to the text, also called stopwords"
      ],
      "metadata": {
        "id": "ldN0JtitvFMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "#Stopwords\n",
        "  stopwords = ['i','me','my','myself','we','our','ours','ourselves','you',\"you're\",\"you've\",\"you'll\",\n",
        "    \"you'd\",'your','yours','yourself','yourselves','he','him','his','himself','she',\n",
        "    \"she's\",'her','hers','herself','it',\"it's\",'its','itself','they','them','their','theirs',\n",
        "    'themselves','what','which','who','whom','this','that',\"that'll\",'these','those','am',\n",
        "    'is','are','was','were','be','been','being','have','has','had','having','do','does',\n",
        "    'did','doing','a','an','the','and','but','if','or','because','as','until','while',\n",
        "    'of','at','by','for','with','about','against','between','into','through','during',\n",
        "    'before','after','above','below','to','from','up','down','in','out','on','off',\n",
        "    'over','under','again','further','then','once','here','there','when','where',\n",
        "    'why','how','all','any','both','each','few','more','most','other','some','such',\n",
        "    'no','nor','not','only','own','same','so','than','too','very','s','t','can',\n",
        "    'will','just','don',\"don't\",'should',\"should've\",'now','d','ll','m','o','re',\n",
        "    've','y','ain','aren',\"aren't\",'couldn',\"couldn't\",'didn',\"didn't\",'doesn',\n",
        "    \"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",'haven',\"haven't\",'isn',\"isn't\",\n",
        "    'ma','mightn',\"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\",\n",
        "    'shouldn',\"shouldn't\",'wasn',\"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\"]\n",
        "  # remove HTML tags\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # remove the characters [\\], ['] and [\"]\n",
        "  text = re.sub(r\"\\\\\", \"\", text)\n",
        "  text = re.sub(r\"\\'\", \"\", text)\n",
        "  text = re.sub(r\"\\\"\", \"\", text)\n",
        "  # convert text to lowercase\n",
        "  text = text.strip().lower()\n",
        "  # replace punctuation characters with spaces\n",
        "  filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "  translate_dict = dict((c, \" \") for c in filters)\n",
        "  translate_map = str.maketrans(translate_dict)\n",
        "  text = text.translate(translate_map)\n",
        "  #removing stopwords\n",
        "  words = text.split()\n",
        "  result = \"\"\n",
        "  for word in words:\n",
        "    if word not in stopwords:\n",
        "      result = result+\" \"+word\n",
        "  text = result\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "gK__v6sZ3Gpq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression on TF-IDF"
      ],
      "metadata": {
        "id": "-n7wAgzbxUx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code will convert the text from the movie reviews to TF-IDF values\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\",preprocessor=clean_text,)\n",
        "#this uses the vectorizer to produce the training and testing tf-idf features for the words\n",
        "training_features = vectorizer.fit_transform(train_data[\"text\"])\n",
        "test_features = vectorizer.transform(test_data[\"text\"])\n",
        "# Training the linear regression model\n",
        "model = LinearRegression()\n",
        "#this plots the training features onto a plane and then plots a linear regression line according to it\n",
        "model.fit(training_features, train_data[\"sentiment\"])\n",
        "y_pred = model.predict(test_features)\n",
        "# decision boundary. Predictions greater than 0.5 are positive and predictions less than 0.5 are negative\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i]<0.5:\n",
        "    y_pred[i] = 0\n",
        "  else:\n",
        "    y_pred[i] = 1\n",
        "# Evaluation\n",
        "#finding the accuracy of the linear regression model on tf-idf values by comparing the predicted sentiments to the actual sentiments\n",
        "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
        "print(acc*100)\n",
        "# confusion matrix creation\n",
        "matrix = confusion_matrix(test_data[\"sentiment\"], y_pred)\n",
        "#the following functions will help format the confusion matrices to make them more visually appealing\n",
        "group_names = ['','','','']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                    matrix.flatten()/np.sum(matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(matrix, annot=labels, fmt='', cmap='Blues')\n"
      ],
      "metadata": {
        "id": "aV2lfaLQvCCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression on TF-IDF\n"
      ],
      "metadata": {
        "id": "nBDbGW0Qx2MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(training_features, train_data[\"sentiment\"])\n",
        "y_pred = model.predict(test_features)\n",
        "# Evaluation\n",
        "#here we continue to use the tf-idf values we found in the previous trial\n",
        "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
        "print(acc*100)\n",
        "#confusion matrix creation\n",
        "matrix = confusion_matrix(test_data[\"sentiment\"], y_pred)\n",
        "group_names = ['','','','']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                    matrix.flatten()/np.sum(matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(matrix, annot=labels, fmt='', cmap='Blues')\n"
      ],
      "metadata": {
        "id": "VoqzP2Mwx3fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression on BOW\n"
      ],
      "metadata": {
        "id": "YAPUc52Zx-Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of using the vectorizer for tf-idf values, we now use countvectorizerv which converts the words to bag of words values\n",
        "vectorizer = CountVectorizer(stop_words=\"english\",preprocessor=clean_text,)\n",
        "training_features = vectorizer.fit_transform(train_data[\"text\"])\n",
        "test_features = vectorizer.transform(test_data[\"text\"])"
      ],
      "metadata": {
        "id": "hY18L24wyBa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the linear regression model according to the BOW values\n",
        "model = LinearRegression()\n",
        "model.fit(training_features, train_data[\"sentiment\"])\n",
        "y_pred = model.predict(test_features)\n",
        "#again, we use the same decision boundary where a predicted sentiment of over 0.5 is considered positive and below 0.5 is considered negative\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i]<0.5:\n",
        "    y_pred[i]=0\n",
        "  else:\n",
        "    y_pred[i]=1\n",
        "# Evaluation\n",
        "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
        "print(acc*100)\n",
        "#confusion matrix\n",
        "matrix = confusion_matrix(test_data[\"sentiment\"], y_pred)\n",
        "group_names = ['','','','']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                    matrix.flatten()/np.sum(matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(matrix, annot=labels, fmt='', cmap='Blues')"
      ],
      "metadata": {
        "id": "noJjZWFkyEAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression on BOW\n"
      ],
      "metadata": {
        "id": "Ab9Yvr7MyMLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the logistic regression model according the bag of words values\n",
        "model = LogisticRegression()\n",
        "model.fit(training_features, train_data[\"sentiment\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
        "print(acc*100)\n",
        "#confusion matrix\n",
        "matrix = confusion_matrix(test_data[\"sentiment\"], y_pred)\n",
        "group_names = ['','','','']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                    matrix.flatten()/np.sum(matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(matrix, annot=labels, fmt='', cmap='Blues')"
      ],
      "metadata": {
        "id": "pGflfpAjyNO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}